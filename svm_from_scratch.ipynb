{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Support Vector Machine</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code of the cost function:\n",
    "$$J(w) = \\frac{1}{2}||w||^2 + C\\left[\\frac{1}{N}\\sum\\limits_{i}^{n}max(0, 1-y_i * (w \\cdotp x_i + b))\\right]$$\n",
    "The first part of the function corresponds to the margin, in fact the width between the two (positive and negative) hyperplanes is equal to: \n",
    "$$width = (x_+ - x_-)* \\frac{w}{||w||}$$ and joining the previous equation with the following equations of the two hyperplanes:\n",
    "$$y_i*(w x_+ + b) -1 = 0$$ and $$ y_i*(w x_- + b) -1 = 0$$ with $$y_i = \\begin{cases}\n",
    "  1 & \\text{for } x_+  \\\\   \n",
    "  -1 & \\text{for } x_-\n",
    "\\end{cases}$$\n",
    "we obtain $$width = \\frac{2}{||w||}$$ and we have to maximize the width which is the same to minimize w and the trick is to transform  $$\\text{min }w$$ into $$ min\\frac{1}{2}||w||^2$$ \n",
    "The second part of the function which begins by C is called Hinge loss function and we have to minimize the sum which corresponds to distance between positive (or negative) hyperplane and our training set. C is a regularization parameter, larger C results in narrow margin and a smaller in a wider margin. N is just the number of lines we have in ours features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(W, X, Y):\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0 # distances = max(0, distances)   \n",
    "    hinge_loss = C * (np.sum(distances) / X.shape[0])\n",
    "    \n",
    "    return 1 / 2 * np.dot(W, W) + hinge_loss ## = cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code of the gradient of the cost function:\n",
    "\n",
    "We can simplify $$max(0, 1-y_i * (w \\cdotp x_i + b))$$ into $$max(0, 1-y_i * (W \\cdotp X_i))$$ with $$W =(w,b)$$ and $$X = (x_i,1)$$\n",
    "and we obtain with the previous simplification:\n",
    "\n",
    "$$J(w) = \\frac{1}{2}||w||^2 + C\\left[\\frac{1}{N}\\sum\\limits_{i}^{n}max(0, 1-y_i * (W \\cdotp X_i))\\right]$$\n",
    "and finally, we have the following gradient of the cost function:\n",
    "\n",
    "$$\\nabla_w J(w) = \\frac{1}{N}\\sum\\limits_{i}^{n}\n",
    "\\begin{cases}\n",
    "  w & \\text{if } max(0, 1-y_i * (W \\cdotp X_i))=0 \\\\   \n",
    "  w-Cy_ix_i    & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function_gradient(W, X, Y):\n",
    "    Y = np.array([Y])\n",
    "    X = np.array([X]) \n",
    "\n",
    "    distances = 1 - (Y * np.dot(X, W))\n",
    "    grad = np.zeros(len(W))\n",
    "\n",
    "    for index, value in enumerate(distances):\n",
    "        if max(0, value) == 0:\n",
    "            dist = W\n",
    "        else:\n",
    "            dist = W - (C * Y[index] * X[index])\n",
    "        grad += dist\n",
    "  \n",
    "    return grad/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to compute the stochastic gradient descent function:\n",
    "For this purpose, we have to minimise the two parts of the following equation:\n",
    "$$J(w) = \\frac{1}{2}||w||^2 + C\\left[\\frac{1}{N}\\sum\\limits_{i}^{n}max(0, 1-y_i * (W \\cdotp X_i))\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_gradient_descent(features, outputs):\n",
    "    max_cycles = 2049\n",
    "    threshold = 0.001 \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    parametre = 0\n",
    "    previous_cost = float(\"inf\")\n",
    "    \n",
    "    cycle = 0\n",
    "    while cycle <max_cycles:\n",
    "        cycle += 1\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for index, value_of_X in enumerate(X):\n",
    "            ascent = compute_cost_function_gradient(weights, value_of_X , Y[index])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "        \n",
    "        if cycle == 2**parametre:\n",
    "            cost = compute_cost(weights, features, outputs)\n",
    "            print(\"nb_of_cycles: {} and Cost: {}\".format(cycle, cost))\n",
    "            parametre +=1\n",
    "            print('cost=', cost)   \n",
    "            if abs(previous_cost - cost) < threshold * previous_cost:\n",
    "                cycle = max_cycles\n",
    "            previous_cost = cost\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id        v1        v2        v3        v4        v5        v6  \\\n",
      "0     A0A2X4V775 -0.009765  0.200064 -0.104441 -0.939137 -0.015482 -0.243657   \n",
      "1     A0A5M9R2F2 -0.044548  0.192516 -0.147199 -0.894085 -0.009355 -0.330328   \n",
      "2     A0A4D6Y563 -0.063157  0.066017 -0.203133 -0.920998 -0.067741 -0.103863   \n",
      "3         P76341  0.036377  0.241014 -0.051348 -0.937244  0.013017 -0.207041   \n",
      "4         A0Z7F9  0.140453  0.229471 -0.070225 -0.960873 -0.028735 -0.324649   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "3995  A0A484GB52 -0.020858  0.087987 -0.106980 -0.970410 -0.041472 -0.208587   \n",
      "3996  A0A5Q4GKP0  0.045559  0.169432 -0.123162 -0.986888 -0.052270 -0.131644   \n",
      "3997  A0A2E4GIK9 -0.056719  0.077032 -0.154847 -0.982715 -0.055467 -0.096901   \n",
      "3998  A0A2M7CQW5  0.015895  0.210303 -0.065212 -0.978872 -0.021216 -0.095097   \n",
      "3999  A0A368CC27  0.020618  0.109883 -0.113834 -0.971576 -0.032970 -0.167549   \n",
      "\n",
      "            v7        v8        v9  ...      v184      v185      v186  \\\n",
      "0     0.118412  0.211521  0.356498  ... -0.055468 -0.116066 -0.031361   \n",
      "1     0.136786  0.223877  0.445563  ...  0.110012 -0.101992  0.008797   \n",
      "2     0.125786  0.234769  0.505547  ...  0.233741 -0.154388  0.137278   \n",
      "3     0.157360  0.192560  0.292982  ...  0.066501 -0.158044  0.086034   \n",
      "4     0.105875  0.147903  0.328881  ...  0.105572 -0.040058 -0.047729   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "3995  0.081806  0.245611  0.339105  ...  0.224782 -0.053446  0.067133   \n",
      "3996  0.070914  0.107376  0.414498  ... -0.013189 -0.002611  0.082772   \n",
      "3997  0.070613  0.146949  0.364325  ...  0.242932 -0.054235 -0.159938   \n",
      "3998  0.143493  0.260738  0.245225  ...  0.518518  0.011767  0.108016   \n",
      "3999  0.075435  0.204833  0.385751  ...  0.203668 -0.007218 -0.079477   \n",
      "\n",
      "          v187       v188      v189      v190      v191      v192  class  \n",
      "0    -0.050158  18.118879 -0.521866 -0.144021  2.924189 -0.278483      1  \n",
      "1    -0.008952  25.939377 -0.765972 -0.096016  3.780535 -0.587374      1  \n",
      "2    -0.054343  10.899848 -0.266273 -0.016061  3.576115  0.046467      1  \n",
      "3    -0.054905  17.253355 -0.334300 -0.147648  3.566811 -0.200707      1  \n",
      "4     0.008716  13.837421 -0.522097 -0.114400  1.796014  0.081138      1  \n",
      "...        ...        ...       ...       ...       ...       ...    ...  \n",
      "3995 -0.116225   7.760444 -0.185658 -0.187799  3.857352  0.009828      2  \n",
      "3996 -0.127592   8.392551 -0.210128 -0.016284  1.087825  0.066868      2  \n",
      "3997  0.185944   4.381233 -0.285145 -0.059999  1.744868  0.299367      2  \n",
      "3998 -0.108312   9.221044 -0.195858 -0.020499  2.877915 -0.033197      2  \n",
      "3999 -0.080238   8.907391 -0.142390 -0.162285  3.299746 -0.044643      2  \n",
      "\n",
      "[4000 rows x 194 columns]\n"
     ]
    }
   ],
   "source": [
    "C = 10000\n",
    "learning_rate = 0.000001\n",
    "#read and display dataset\n",
    "data = pd.read_csv('merge_cyto_periplasm.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id        v1        v2        v3        v4        v5        v6  \\\n",
      "0     A0A2X4V775 -0.009765  0.200064 -0.104441 -0.939137 -0.015482 -0.243657   \n",
      "1     A0A5M9R2F2 -0.044548  0.192516 -0.147199 -0.894085 -0.009355 -0.330328   \n",
      "2     A0A4D6Y563 -0.063157  0.066017 -0.203133 -0.920998 -0.067741 -0.103863   \n",
      "3         P76341  0.036377  0.241014 -0.051348 -0.937244  0.013017 -0.207041   \n",
      "4         A0Z7F9  0.140453  0.229471 -0.070225 -0.960873 -0.028735 -0.324649   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "3995  A0A484GB52 -0.020858  0.087987 -0.106980 -0.970410 -0.041472 -0.208587   \n",
      "3996  A0A5Q4GKP0  0.045559  0.169432 -0.123162 -0.986888 -0.052270 -0.131644   \n",
      "3997  A0A2E4GIK9 -0.056719  0.077032 -0.154847 -0.982715 -0.055467 -0.096901   \n",
      "3998  A0A2M7CQW5  0.015895  0.210303 -0.065212 -0.978872 -0.021216 -0.095097   \n",
      "3999  A0A368CC27  0.020618  0.109883 -0.113834 -0.971576 -0.032970 -0.167549   \n",
      "\n",
      "            v7        v8        v9  ...      v184      v185      v186  \\\n",
      "0     0.118412  0.211521  0.356498  ... -0.055468 -0.116066 -0.031361   \n",
      "1     0.136786  0.223877  0.445563  ...  0.110012 -0.101992  0.008797   \n",
      "2     0.125786  0.234769  0.505547  ...  0.233741 -0.154388  0.137278   \n",
      "3     0.157360  0.192560  0.292982  ...  0.066501 -0.158044  0.086034   \n",
      "4     0.105875  0.147903  0.328881  ...  0.105572 -0.040058 -0.047729   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "3995  0.081806  0.245611  0.339105  ...  0.224782 -0.053446  0.067133   \n",
      "3996  0.070914  0.107376  0.414498  ... -0.013189 -0.002611  0.082772   \n",
      "3997  0.070613  0.146949  0.364325  ...  0.242932 -0.054235 -0.159938   \n",
      "3998  0.143493  0.260738  0.245225  ...  0.518518  0.011767  0.108016   \n",
      "3999  0.075435  0.204833  0.385751  ...  0.203668 -0.007218 -0.079477   \n",
      "\n",
      "          v187       v188      v189      v190      v191      v192  class  \n",
      "0    -0.050158  18.118879 -0.521866 -0.144021  2.924189 -0.278483    1.0  \n",
      "1    -0.008952  25.939377 -0.765972 -0.096016  3.780535 -0.587374    1.0  \n",
      "2    -0.054343  10.899848 -0.266273 -0.016061  3.576115  0.046467    1.0  \n",
      "3    -0.054905  17.253355 -0.334300 -0.147648  3.566811 -0.200707    1.0  \n",
      "4     0.008716  13.837421 -0.522097 -0.114400  1.796014  0.081138    1.0  \n",
      "...        ...        ...       ...       ...       ...       ...    ...  \n",
      "3995 -0.116225   7.760444 -0.185658 -0.187799  3.857352  0.009828   -1.0  \n",
      "3996 -0.127592   8.392551 -0.210128 -0.016284  1.087825  0.066868   -1.0  \n",
      "3997  0.185944   4.381233 -0.285145 -0.059999  1.744868  0.299367   -1.0  \n",
      "3998 -0.108312   9.221044 -0.195858 -0.020499  2.877915 -0.033197   -1.0  \n",
      "3999 -0.080238   8.907391 -0.142390 -0.162285  3.299746 -0.044643   -1.0  \n",
      "\n",
      "[4000 rows x 194 columns]\n"
     ]
    }
   ],
   "source": [
    "#convert existing labels into 1 et -1 labels\n",
    "diag_map = {1: 1.0, 2: -1.0}\n",
    "data['class'] = data['class'].map(diag_map)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign labels and features to different data frames\n",
    "Y = data.loc[:, 'class']\n",
    "X = data.iloc[:, 1:193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize values of features to avoid overflow\n",
    "X_normalized = MinMaxScaler().fit_transform(X.values)\n",
    "X = pd.DataFrame(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a new column b full of 1 at the end\n",
    "X.insert(loc=len(X.columns), column='b', value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset to obtain train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n",
      "nb_of_cycles: 1 and Cost: 1489.465543350465\n",
      "cost= 1489.465543350465\n",
      "nb_of_cycles: 2 and Cost: 643.2796441834901\n",
      "cost= 643.2796441834901\n",
      "nb_of_cycles: 4 and Cost: 505.56386752871583\n",
      "cost= 505.56386752871583\n",
      "nb_of_cycles: 8 and Cost: 425.0928885389048\n",
      "cost= 425.0928885389048\n",
      "nb_of_cycles: 16 and Cost: 491.00683933252776\n",
      "cost= 491.00683933252776\n",
      "nb_of_cycles: 32 and Cost: 324.160785506771\n",
      "cost= 324.160785506771\n",
      "nb_of_cycles: 64 and Cost: 295.6788924368547\n",
      "cost= 295.6788924368547\n",
      "nb_of_cycles: 128 and Cost: 316.6418634900643\n",
      "cost= 316.6418634900643\n",
      "nb_of_cycles: 256 and Cost: 417.1326230161585\n",
      "cost= 417.1326230161585\n",
      "nb_of_cycles: 512 and Cost: 257.9205352225961\n",
      "cost= 257.9205352225961\n",
      "nb_of_cycles: 1024 and Cost: 321.94337948435555\n",
      "cost= 321.94337948435555\n",
      "nb_of_cycles: 2048 and Cost: 258.71147576898863\n",
      "cost= 258.71147576898863\n",
      "training end\n",
      "weights are: [ 7.53975374e-01  2.77020464e+00 -1.20099290e+00 -9.92648890e-01\n",
      "  1.59637719e+00 -1.63613151e-01  9.80829356e-01 -2.49927309e+00\n",
      "  3.40803938e-01 -2.90903123e-01 -2.73852678e-01 -8.94877855e-01\n",
      "  9.83397129e-01 -3.05206634e+00  8.30932669e-01  1.53828344e+00\n",
      "  2.65562294e-01  6.87732873e-01  4.24642800e-01 -3.61816768e-01\n",
      " -9.70904013e-01 -2.07900539e-01 -1.73786514e+00  1.63168869e+00\n",
      " -1.16391533e+00  2.54510825e+00 -1.06848903e+00 -2.35497386e+00\n",
      "  8.04328059e-01  7.60534240e-01 -2.35454773e+00  4.76751802e-01\n",
      "  2.21498436e+00 -2.64360690e+00 -1.38546468e+00  2.27338516e+00\n",
      " -1.30101486e+00  2.05727367e+00 -6.66762212e-02 -1.99920038e+00\n",
      " -1.77719767e-01  1.88171598e+00  4.55457482e-01  2.03875653e+00\n",
      "  1.68877604e+00  3.39931081e-01 -1.15484615e+00  8.32429085e-02\n",
      " -2.95014821e-01  7.84897470e-02  5.79458366e-01  2.41685986e+00\n",
      "  2.98395043e-01  2.35228563e-01 -8.36735980e-01  2.73408222e+00\n",
      "  1.57984943e-01  6.85581451e-01  7.82092453e-01  5.31339991e-01\n",
      " -1.36738477e+00 -2.29957871e+00  2.63828484e+00  1.31560800e+00\n",
      "  9.13731470e-03  1.34911876e-01 -9.61249174e-01  5.65350716e-01\n",
      " -3.35081821e-01  5.14964198e-01  1.91576133e-01  3.54687893e-01\n",
      " -6.05692083e-01 -5.67647578e-01  6.11511630e-01  3.26343447e-02\n",
      " -3.53786631e-01 -3.08261230e-01  1.91316446e-01  7.49999040e-03\n",
      "  4.66631242e-02  1.06470502e-01  1.47089655e-01  4.00837329e-01\n",
      " -1.14272009e+00  1.99069705e-01  1.42176109e-01  3.73401488e-01\n",
      " -2.77257088e-01  4.12236868e-01 -3.20544560e-01  3.41973400e-01\n",
      "  9.24186544e-02  4.88586085e-01 -3.28829155e-01 -5.51946091e-01\n",
      "  1.29308063e+00 -4.29754418e-02  6.90992593e-01  3.12059082e-01\n",
      "  5.72007294e-01 -8.81894711e-01  3.09568802e-01  5.38525751e-02\n",
      "  4.64234861e-01 -6.20640514e-01  2.13087489e-01 -4.84920328e-01\n",
      " -1.77776985e-01  3.94355863e-02 -1.54087358e-01 -6.61430594e-01\n",
      "  7.46803759e-01  2.16517247e-01  7.23267874e-01  7.11780368e-01\n",
      "  1.13047792e-01 -1.19109002e+00  2.45673640e-01  9.01645177e-02\n",
      "  5.36476513e-01 -2.99187293e-01 -4.54616661e-01 -5.68780006e-01\n",
      " -1.58944706e+00 -1.69735818e-01 -1.74346076e-02  6.72904307e-03\n",
      " -1.63219801e-01 -5.59022205e-01 -1.09835754e-01 -2.55266555e-01\n",
      " -5.21079930e-01  3.57230470e-01  2.62680054e-01 -9.45734177e-01\n",
      " -2.05377822e-01 -6.09602080e-01  4.00934671e-01 -7.05166373e-01\n",
      "  1.41578042e-01 -4.48736846e-01  3.81347553e-01  1.24955766e+00\n",
      " -9.78315544e-01  7.45820804e-02  9.68176759e-01 -6.65683174e-01\n",
      " -1.10632999e+00  1.72266724e+00 -9.36268063e-01  2.38737702e-01\n",
      " -1.08804846e+00 -1.37648329e-01  8.31760806e-02  2.34443448e-01\n",
      " -5.89281238e-01 -5.58335157e-01 -8.41967951e-01 -6.98173669e-01\n",
      " -7.56891710e-01  6.47094727e-01 -9.11327817e-01 -2.20656413e-01\n",
      " -2.05524691e-01 -1.00899662e+00 -1.65950896e-01  2.95666040e-01\n",
      "  3.96208070e-01  5.87789978e-01  4.43396670e-01  2.92500235e-01\n",
      "  3.26645913e-01 -1.63628503e-01 -8.47293627e-01 -1.46065897e-01\n",
      "  1.70204547e-03 -5.46360630e-01  3.88332045e-01  9.74251240e-01\n",
      " -3.08749890e-01  2.77396142e-02  9.10071967e-01 -6.17524962e-01\n",
      "  5.01721092e-01 -8.64904848e-02  3.27600149e-02  4.92307728e-01\n",
      " -4.54142803e-01 -4.42827243e-01 -3.72530701e-02  2.53133532e-01\n",
      "  3.93760284e-01]\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "print(\"training start\")\n",
    "W = compute_stochastic_gradient_descent(X_train.to_numpy(), y_train.to_numpy())\n",
    "print(\"training end\")\n",
    "print(\"weights are: {}\".format(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the model...\n",
      "accuracy of svm on test dataset: 0.98375\n",
      "precision of svm on test dataset: 0.983451536643026\n"
     ]
    }
   ],
   "source": [
    "print(\"testing the model...\")\n",
    "y_train_predicted = np.array([])\n",
    "for i in range(X_train.shape[0]):\n",
    "    yp = np.sign(np.dot(X_train.to_numpy()[i], W))\n",
    "    y_train_predicted = np.append(y_train_predicted, yp)\n",
    "\n",
    "y_test_predicted = np.array([])\n",
    "for i in range(X_test.shape[0]):\n",
    "    yp = np.sign(np.dot(X_test.to_numpy()[i], W))\n",
    "    y_test_predicted = np.append(y_test_predicted, yp)\n",
    "\n",
    "print(\"accuracy of svm on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "print(\"precision of svm on test dataset: {}\".format(precision_score(y_test, y_test_predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
