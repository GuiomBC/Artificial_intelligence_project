{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shallow_nn as nn\n",
    "import optimized_tree as tr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser for the unirep files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unirep(file_path):\n",
    "    \"\"\"\n",
    "    Parse a Unirep file.\n",
    "    \"\"\"\n",
    "    data_matrix = []\n",
    "    with open(file_path) as fp:\n",
    "        data = fp.readlines()\n",
    "    for i, line in enumerate(data):\n",
    "        if line[0] == '>':\n",
    "            data_matrix.append(np.array(\n",
    "                data[i + 1].strip().split(sep=' '),\n",
    "                dtype=np.float64))\n",
    "    return np.array(data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will generate a dataset composed of 6000 samples: 3000 cytoplasmic proteins, and 3000 periplasmic proteins. The dataset for each class is first shuffled, then the 3000 first elements of each set are extracted onto the final dataset. This process is equivalent as random selection of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports unirep files.\n",
    "raw_cyt = read_unirep(\"cytoxplasmUniRef50(1).unirep\")\n",
    "raw_peri = read_unirep(\"periplasmUniRef50(1).unirep\")\n",
    "# Shuffle imported data.\n",
    "np.random.shuffle(raw_cyt)\n",
    "np.random.shuffle(raw_peri)\n",
    "# Creates mixed dataset (from shuffled = random selection).\n",
    "all_raw = np.concatenate((raw_cyt[0:3000], raw_peri[0:3000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we scale the data, giving for each feature a mean = 0, and a standard deviation = 1. This makes the learning process easier for certain type of algorithms, for example for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use sklearn, which allows to get the scaling factor \n",
    "# and the mean used to scale the initial data.\n",
    "\n",
    "# Scaler object init and fit to data.\n",
    "scaler = StandardScaler().fit(all_raw)\n",
    "# Apply scaler to the training set.\n",
    "scaled_transfrom = scaler.transform(all_raw)\n",
    "# Extracts and save scaling parameters.\n",
    "sc = scaler.scale_\n",
    "mn = scaler.mean_\n",
    "np.save('scale_factor.npy' , sc)\n",
    "np.save('scale_mean.npy', mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.469446951953614e-18, 1.734723475976807e-18]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' All comparisons considered as not equal are actually really close '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We transform data back to see if it works as expected.\n",
    "back_transform = scaled_transfrom * sc + mn\n",
    "equality_check = [elt - tle for elt, tle in zip(back_transform[0], all_raw[0]) \n",
    "                  if elt != tle]\n",
    "print(equality_check)\n",
    "\"\"\" All comparisons considered as not equal are actually really close \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate the labels for the dataset. The labels are kept in an array of the size of the dataset, and are given the value 1 for cytoplasmic proteins, or 0 for periplasmic ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives value 1 to cytoplasmic proteins, 0 to others.\n",
    "labels = np.zeros(len(all_raw), dtype=int)\n",
    "labels[0:3000] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network we designed needs a one-hot encoding for the labels. One-hot encoding means that the value is stored in a sparse array, where all the values are 0 except the one corresponding the the class. For example, if there are 5 classes, an item with class 3 will be encoded as follows: [0, 0, 0, 1, 0, 0]. We designed a function that transfroms an array of integers to that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for neural network.\n",
    "def one_hot(labels):\n",
    "    encoded_labels = []\n",
    "    number_classes = len(set(labels))\n",
    "    for item in labels:\n",
    "        encoded = np.zeros(number_classes, dtype=int)\n",
    "        encoded[item] = 1\n",
    "        encoded_labels.append(encoded)\n",
    "    return np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_labels = one_hot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the way the network is designed requires the data corresponding to a certain sample to be stored with it's label in an array, giving a dataset on the from [[sample1_values, sample1_label], [sample2_values, sample2_label], ...]. The following function adapts the data to this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares input for neural network.\n",
    "def nn_input_format(dataset, labels):\n",
    "    encoded_data = []\n",
    "    for data, lab in zip(dataset, labels):\n",
    "        value = (data.reshape((-1,1)), lab.reshape((-1,1)))\n",
    "        encoded_data.append(value)\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn_input_format(scaled_transfrom, f_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network initialisation and training.\n",
    "final_model = nn.ShallowNetwork([64, 22, 2])\n",
    "final_model.stochastic_gradient_descent(x, 100, 8, 0.2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('weights.npy' , final_model.weights)\n",
    "np.save('biases.npy', final_model.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
